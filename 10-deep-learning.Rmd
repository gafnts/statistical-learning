# Deep learning

```{r}
pacman::p_load(tidyverse, tidymodels, ISLR2, keras)
```

## A single layer network on the Hitters data

```{r}
gitters <- Hitters |> as_tibble() |> drop_na()
```

```{r}
set.seed(1)
gitters_split <- initial_split(prop = 2/3, gitters)
gitters_train <- training(gitters_split)
gitters_test <- testing(gitters_split)
```

#### Linear model

```{r}
lm_fit <- linear_reg() |> fit(Salary ~ ., data = gitters_train)
lm_fit |> pluck('fit') |> summary()
```

```{r}
lm_predict <- 
  bind_cols(
    gitters_test |> select(Salary),
    predict(lm_fit, new_data = gitters_test),
    predict(lm_fit, new_data = gitters_test, type = 'conf_int')
  )

mae(lm_predict, Salary, .pred)
```

#### Lasso regularization

```{r}
glm_spec <- 
  linear_reg(
    penalty = tune(),
    mixture = 1
    ) |> 
  set_engine("glmnet")

glm_rec <- 
  gitters_train |> 
  recipe('Salary ~ . + 0') |> 
  step_normalize(all_numeric(), -all_outcomes()) |> 
  step_dummy(all_nominal())

glm_wf <- 
  workflow() |> 
  add_model(glm_spec) |> 
  add_recipe(glm_rec)

folds <- vfold_cv(gitters_train, v = 5)

grid <- 
  grid_random(
    penalty(), 
    size = 100
  )
  
doParallel::registerDoParallel()

metrics <- metric_set(mae, rsq)

glm_grid <- 
  tune_grid(
    glm_wf,
    resamples = folds,
    grid = grid,
    metrics = metrics
)

glm_grid |> 
  collect_metrics() |> 
  ggplot(aes(penalty, mean)) +
  geom_point() +
  facet_wrap(vars(.metric), scales = 'free_y')

glm_final <- 
  finalize_workflow(
    glm_wf,
    glm_grid |> select_best('mae')
  )

last_fit(
  glm_final,
  gitters_split,
  metrics = metrics
) |> 
  collect_metrics()
```

#### Neural network

```{r}
set.seed(2)
n <- nrow(gitters)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
x <- model.matrix(Salary ~ . - 1, data = gitters) |> scale()
y <- gitters$Salary

modnn <- 
  keras_model_sequential() |> 
  layer_dense(units = 50, activation = "relu", input_shape = ncol(x)) |> 
  layer_dropout(rate = 0.4)|> 
  layer_dense(units = 1)

modnn |> 
  compile(loss = "mse",
           optimizer = optimizer_rmsprop(),
          metrics = list("mean_absolute_error")
          )

history <- 
  modnn |> 
  fit(x[-testid, ], y[-testid],
      epochs = 1500, batch_size = 32, 
      validation_data = list(x[testid, ], y[testid])
      )
```

```{r}
plot(history)
```

```{r}
npred <- predict(modnn, x[testid, ])
mean(abs(y[testid] - npred))
```

## A multilayer network on the MNIST digit data

```{r}
mnist <- dataset_mnist()

x_train <- mnist$train$x 
g_train <- mnist$train$y 
x_test <- mnist$test$x
g_test <- mnist$test$y 

dim(x_train)
dim(x_test)
```

```{r}
x_train <- array_reshape(x_train, c(nrow(x_train), 784)) 
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
y_train <- to_categorical(g_train, 10)
y_test <- to_categorical(g_test, 10)
```

```{r}
x_train <- x_train / 255 
x_test <- x_test / 255
```

```{r}
modelnn <- keras_model_sequential() 

modelnn  |> 
  layer_dense(units = 256, 
              activation = "relu",
              input_shape = c(784)) |>
  layer_dropout(rate = 0.4) |>
  layer_dense(units = 128, activation = "relu") |>
  layer_dropout(rate = 0.3) |>
  layer_dense(units = 10, activation = "softmax")

modelnn |> 
  compile(loss = "categorical_crossentropy", 
          optimizer = optimizer_rmsprop(), 
          metrics = c("accuracy")
  )

system.time(
  history <- 
    modelnn |>
      fit(x_train, 
          y_train, 
          epochs = 30, 
          batch_size = 128, 
          validation_split = 0.2) 
  )

plot(history, smooth = FALSE)
```

```{r}
accuracy <- function(pred, truth) 
  mean(drop(pred) == drop(truth))

modelnn |> predict(x_test) |> accuracy(g_test)
```

```{r}
modellr <- 
  keras_model_sequential() |>
  layer_dense(input_shape = 784, units = 10,
              activation = "softmax")

summary(modellr)

modellr |> 
  compile(loss = "categorical_crossentropy", 
          optimizer = optimizer_rmsprop(), 
          metrics = c("accuracy"))

modellr |> 
  fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
```

```{r}
modellr |> predict(x_test) |> accuracy(g_test)
```
