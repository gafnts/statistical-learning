# Deep learning

```{r}
pacman::p_load(tidyverse, tidymodels, ISLR2, keras)
```

## A single layer network on the Hitters data

```{r}
gitters <- Hitters |> as_tibble() |> drop_na()
```

```{r}
set.seed(1)
gitters_split <- initial_split(prop = 2/3, gitters)
gitters_train <- training(gitters_split)
gitters_test <- testing(gitters_split)
```

#### Linear model

```{r}
lm_fit <- linear_reg() |> fit(Salary ~ ., data = gitters_train)
lm_fit |> pluck('fit') |> summary()
```

```{r}
lm_predict <- 
  bind_cols(
    gitters_test |> select(Salary),
    predict(lm_fit, new_data = gitters_test),
    predict(lm_fit, new_data = gitters_test, type = 'conf_int')
  )

mae(lm_predict, Salary, .pred)
```

#### Lasso regularization

```{r}
glm_spec <- 
  linear_reg(
    penalty = tune(),
    mixture = 1
    ) |> 
  set_engine("glmnet")

glm_rec <- 
  gitters_train |> 
  recipe('Salary ~ . + 0') |> 
  step_normalize(all_numeric(), -all_outcomes()) |> 
  step_dummy(all_nominal())

glm_wf <- 
  workflow() |> 
  add_model(glm_spec) |> 
  add_recipe(glm_rec)

folds <- vfold_cv(gitters_train, v = 5)

grid <- 
  grid_random(
    penalty(), 
    size = 100
  )
  
doParallel::registerDoParallel()

metrics <- metric_set(mae, rsq)

glm_grid <- 
  tune_grid(
    glm_wf,
    resamples = folds,
    grid = grid,
    metrics = metrics
)

glm_grid |> 
  collect_metrics() |> 
  ggplot(aes(penalty, mean)) +
  geom_point() +
  facet_wrap(vars(.metric), scales = 'free_y')

glm_final <- 
  finalize_workflow(
    glm_wf,
    glm_grid |> select_best('mae')
  )

last_fit(
  glm_final,
  gitters_split,
  metrics = metrics
) |> 
  collect_metrics()
```

#### Neural network

```{r}
set.seed(2)
n <- nrow(gitters)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
x <- model.matrix(Salary ~ . - 1, data = gitters) |> scale()
y <- gitters$Salary

modnn <- 
  keras_model_sequential() |> 
  layer_dense(units = 50, activation = "relu", input_shape = ncol(x)) |> 
  layer_dropout(rate = 0.4)|> 
  layer_dense(units = 1)

modnn |> 
  compile(loss = "mse",
           optimizer = optimizer_rmsprop(),
          metrics = list("mean_absolute_error")
          )

history <- 
  modnn |> 
  fit(x[-testid, ], y[-testid],
      epochs = 1500, batch_size = 32, 
      validation_data = list(x[testid, ], y[testid])
      )
```

```{r}
plot(history)
```

```{r}
npred <- predict(modnn, x[testid, ])
mean(abs(y[testid] - npred))
```
